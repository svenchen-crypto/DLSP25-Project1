{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIugLjz-A2Qd"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from torch.optim import lr_scheduler # StepLR, CosineAnnealingLR, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from data_loader import get_cifar10_dataloaders\n",
    "from trainer import train_model\n",
    "from model import ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4z7iY1pkk2C"
   },
   "source": [
    "Configure the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e3wMn_41kd5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the total number of trainable parameters\n",
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4903242"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "num_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_study_details(checkpoint_dir, trial_num, trial_details):\n",
    "    file_path = os.path.join(checkpoint_dir, \"study_details.json\")\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, \"w\") as f:\n",
    "            json.dump({}, f, indent=4)\n",
    "    \n",
    "    with open(file_path, \"r\") as f:\n",
    "        study_details = json.load(f)\n",
    "\n",
    "    study_details[str(trial_num)] = trial_details\n",
    "    \n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(study_details, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    study_name = trial.study.study_name\n",
    "    checkpoint_dir = f\"checkpoints_{study_name}\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True) # Create a directory for checkpoints if it doesn't exist\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_epochs = 150 # trial.suggest_int(\"num_epochs\", 20, 35)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128]) # Rmed: 256\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer_type\", [\"Adam\", \"SGD\"]) # Rmed: RMSprop\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"OneCycleLR\"]) # Rmed: StepLR\n",
    "\n",
    "    optimizer_map = {\n",
    "        \"Adam\": optim.AdamW,\n",
    "        \"SGD\": optim.SGD,\n",
    "        \"RMSprop\": optim.RMSprop\n",
    "    }\n",
    "\n",
    "    scheduler_map = {\n",
    "        \"StepLR\": lr_scheduler.StepLR,\n",
    "        \"CosineAnnealingLR\": lr_scheduler.CosineAnnealingLR,\n",
    "        \"ReduceLROnPlateau\": lr_scheduler.ReduceLROnPlateau,\n",
    "        \"OneCycleLR\": lr_scheduler.OneCycleLR\n",
    "    }\n",
    "    \n",
    "    optimizer_params = {\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [1e-4, 5e-4])\n",
    "    }\n",
    "    \n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True)\n",
    "        optimizer_params[\"momentum\"] = 0.9 # trial.suggest_float(\"momentum\", 0.8, 0.9)\n",
    "        optimizer_params[\"nesterov\"] = True #bool(trial.suggest_categorical(\"nesterov\", [0, 1]))\n",
    "        optimizer_params[\"weight_decay\"] = 5e-4\n",
    "    else:\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.0001, 0.001, log=True)\n",
    "        optimizer_params[\"weight_decay\"] = 1e-4\n",
    "        \n",
    "    # Suggest data transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(trial.suggest_float(\"h_flip\", 0.0, 1.0)),\n",
    "        transforms.RandomRotation(trial.suggest_int(\"rotation\", 0, 30)),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize with mean and std of CIFAR-10\n",
    "    ])\n",
    "    \n",
    "    train_loader, valid_loader = get_cifar10_dataloaders(\n",
    "        transform,\n",
    "        subset_percent=1, \n",
    "        valid_size=0.1,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4\n",
    "    )\n",
    "\n",
    "    scheduler_params = {}\n",
    "    if scheduler_type == \"StepLR\":\n",
    "        scheduler_params[\"step_size\"] = trial.suggest_int(\"step_size\", 5, 20)\n",
    "        scheduler_params[\"gamma\"] = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "    elif scheduler_type == \"CosineAnnealingLR\":\n",
    "        scheduler_params[\"T_max\"] = num_epochs #trial.suggest_int(\"T_max\", 10, 50)\n",
    "        scheduler_params[\"eta_min\"] = trial.suggest_float(\"eta_min\", 0.0, 1e-6)\n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        scheduler_params[\"factor\"] = trial.suggest_float(\"factor\", 0.1, 0.9)\n",
    "        scheduler_params[\"patience\"] = trial.suggest_int(\"patience\", 2, 10)\n",
    "        scheduler_params[\"mode\"] = \"min\"\n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        scheduler_params[\"max_lr\"] = 0.1\n",
    "        scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n",
    "        scheduler_params[\"epochs\"] = num_epochs\n",
    "\n",
    "    # Define model\n",
    "    model = ResNet18()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    trial_details = trial.params.copy()\n",
    "    trial_details[\"model_name\"] =  \"resnet18\"\n",
    "    trial_details[\"trainable_parameters\"] = num_params(model)\n",
    "    \n",
    "    # Print trial details\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{trial.number=}\")\n",
    "    for param, val in trial_details.items():\n",
    "        print(f\"{param}: {val}\")\n",
    "    print(\"- \" * 25)\n",
    "    update_study_details(checkpoint_dir, trial.number, trial_details)\n",
    "\n",
    "    optimizer = optimizer_map[optimizer_type](model.parameters(), **optimizer_params)\n",
    "    scheduler = scheduler_map[scheduler_type](optimizer, **scheduler_params)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training\n",
    "    best_val_accuracy = train_model(\n",
    "        trial, model, train_loader, criterion, optimizer, \n",
    "        valid_loader=valid_loader, num_epochs=num_epochs, device=device,\n",
    "        scheduler=scheduler\n",
    "    )\n",
    "\n",
    "    # Checkpoint the model with the best validation accuracy\n",
    "    model_filename = f\"model_trial_{trial.number}_val_acc_{best_val_accuracy:.4f}.pth\"\n",
    "    model_path = os.path.join(checkpoint_dir, model_filename)\n",
    "    \n",
    "    # Save the model state_dict\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model checkpoint saved to {model_path}\")\n",
    "    trial_details[\"best_val_accuracy\"] = best_val_accuracy\n",
    "    trial_details[\"checkpoint_path\"] = model_path\n",
    "    update_study_details(checkpoint_dir, trial.number, trial_details)\n",
    "    \n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 02:23:41,767] A new study created in memory with name: study_2025-03-10_02-23-41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "trial.number=0\n",
      "batch_size: 64\n",
      "optimizer_type: Adam\n",
      "scheduler_type: OneCycleLR\n",
      "weight_decay: 0.0001\n",
      "learning_rate: 0.0006361471280328357\n",
      "h_flip: 0.37619653334953174\n",
      "rotation: 21\n",
      "model_name: resnet18\n",
      "trainable_parameters: 4903242\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "  Epoch [1/20], Batch [700/704], Train Acc: 0.3321 Loss: 1.6405\n",
      "  Validation Accuracy after Epoch 1: 0.4142\n",
      "  Epoch [2/20], Batch [700/704], Train Acc: 0.4540 Loss: 1.5143\n",
      "  Validation Accuracy after Epoch 2: 0.4498\n",
      "  Epoch [3/20], Batch [700/704], Train Acc: 0.4957 Loss: 1.5282\n",
      "  Validation Accuracy after Epoch 3: 0.5304\n",
      "  Epoch [4/20], Batch [700/704], Train Acc: 0.5027 Loss: 1.6859\n",
      "  Validation Accuracy after Epoch 4: 0.4598\n",
      "  Epoch [5/20], Batch [700/704], Train Acc: 0.5141 Loss: 1.0712\n",
      "  Validation Accuracy after Epoch 5: 0.5288\n",
      "  Epoch [6/20], Batch [700/704], Train Acc: 0.5305 Loss: 1.3236\n",
      "  Validation Accuracy after Epoch 6: 0.5288\n",
      "  Epoch [7/20], Batch [700/704], Train Acc: 0.5385 Loss: 1.1012\n",
      "  Validation Accuracy after Epoch 7: 0.5550\n",
      "  Epoch [8/20], Batch [700/704], Train Acc: 0.5734 Loss: 1.2904\n",
      "  Validation Accuracy after Epoch 8: 0.5718\n",
      "  Epoch [9/20], Batch [700/704], Train Acc: 0.5954 Loss: 0.9408\n",
      "  Validation Accuracy after Epoch 9: 0.6366\n",
      "  Epoch [10/20], Batch [700/704], Train Acc: 0.6201 Loss: 0.8491\n",
      "  Validation Accuracy after Epoch 10: 0.6600\n",
      "  Epoch [11/20], Batch [700/704], Train Acc: 0.6476 Loss: 0.8780\n",
      "  Validation Accuracy after Epoch 11: 0.6126\n",
      "  Epoch [12/20], Batch [700/704], Train Acc: 0.6681 Loss: 1.3402\n",
      "  Validation Accuracy after Epoch 12: 0.6854\n",
      "  Epoch [13/20], Batch [700/704], Train Acc: 0.6910 Loss: 0.7645\n",
      "  Validation Accuracy after Epoch 13: 0.7072\n",
      "  Epoch [14/20], Batch [700/704], Train Acc: 0.7138 Loss: 0.6415\n",
      "  Validation Accuracy after Epoch 14: 0.7536\n",
      "  Epoch [15/20], Batch [700/704], Train Acc: 0.7345 Loss: 0.9229\n",
      "  Validation Accuracy after Epoch 15: 0.7518\n",
      "  Epoch [16/20], Batch [700/704], Train Acc: 0.7545 Loss: 0.8563\n",
      "  Validation Accuracy after Epoch 16: 0.7712\n",
      "  Epoch [17/20], Batch [700/704], Train Acc: 0.7706 Loss: 0.8891\n",
      "  Validation Accuracy after Epoch 17: 0.7888\n",
      "  Epoch [18/20], Batch [700/704], Train Acc: 0.7823 Loss: 0.4960\n",
      "  Validation Accuracy after Epoch 18: 0.7932\n",
      "  Epoch [19/20], Batch [700/704], Train Acc: 0.7887 Loss: 0.6554\n",
      "  Validation Accuracy after Epoch 19: 0.7982\n",
      "  Epoch [20/20], Batch [700/704], Train Acc: 0.7921 Loss: 0.5700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-10 02:28:48,733] Trial 0 finished with value: 0.8034 and parameters: {'batch_size': 64, 'optimizer_type': 'Adam', 'scheduler_type': 'OneCycleLR', 'weight_decay': 0.0001, 'learning_rate': 0.0006361471280328357, 'h_flip': 0.37619653334953174, 'rotation': 21}. Best is trial 0 with value: 0.8034.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Validation Accuracy after Epoch 20: 0.8034\n",
      "Trial 0 complete. Best Validation Accuracy: 0.8034\n",
      "\n",
      "Model checkpoint saved to checkpoints_study_2025-03-10_02-23-41/model_trial_0_val_acc_0.8034.pth\n",
      "Best trial: 0\n",
      "Best hyperparameters: {'batch_size': 64, 'optimizer_type': 'Adam', 'scheduler_type': 'OneCycleLR', 'weight_decay': 0.0001, 'learning_rate': 0.0006361471280328357, 'h_flip': 0.37619653334953174, 'rotation': 21}\n",
      "Best validation accuracy: 0.8034\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "study_name = f\"study_{timestamp}\"    \n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=study_name)\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet18()\n",
    "model.to(device)\n",
    "\n",
    "checkpoint_dir = f\"checkpoints_{study_name}\"\n",
    "with open(os.path.join(checkpoint_dir, \"study_details.json\"), \"r\") as f:\n",
    "    study_details = json.load(f)\n",
    "best_checkpoint_fp = study_details[str(study.best_trial.number)][\"checkpoint_path\"]\n",
    "# best_checkpoint_fp = \"\"\n",
    "\n",
    "# Load the latest checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_fp)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8394\n"
     ]
    }
   ],
   "source": [
    "from trainer import evaluate_model\n",
    "from data_loader import get_test_dataloader\n",
    "\n",
    "test_loader = get_test_dataloader()\n",
    "acc = evaluate_model(model, test_loader, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on cifar10.1 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.74\n"
     ]
    }
   ],
   "source": [
    "from cifar10_1_dataloader import get_dataloader_10_1\n",
    "dataloader_10_1 = get_dataloader_10_1()\n",
    "\n",
    "acc = evaluate_model(model, dataloader_10_1, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_kaggle_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate submission file with test data\n",
    "kaggle_test_loader = get_kaggle_test_dataloader()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, in kaggle_test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "# submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kaggle\n",
    "# kaggle.api.competition_submit(\n",
    "#     file_name=\"submission.csv\",\n",
    "#     message=\"0.9237\",\n",
    "#     competition=competition_name\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPk8i7jiGjSg0feqDTW0l2u",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
