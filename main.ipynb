{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIugLjz-A2Qd"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gk2657/DLSP25-Project1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from data_loader import get_cifar10_dataloaders, get_test_dataloader, get_kaggle_test_dataloader\n",
    "from helper import optimizer_map, scheduler_map, num_params, update_study_details\n",
    "from models import BaseResNet, EfficientNetB0, SmallResNet0\n",
    "from trainer import train_model\n",
    "from run import single_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4z7iY1pkk2C"
   },
   "source": [
    "Configure the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "e3wMn_41kd5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Trainable parameters: 2998402\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = SmallResNet0()\n",
    "print(\"Trainable parameters:\", num_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    study_dir = f\"studies/{trial.study.study_name}\"\n",
    "    os.makedirs(study_dir, exist_ok=True) # Create a directory for checkpoints if it doesn't exist\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_epochs = 150 # trial.suggest_int(\"num_epochs\", 20, 35)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128]) # Rmed: 256\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer_type\", [\"Adam\", \"SGD\"]) # Rmed: RMSprop\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"OneCycleLR\"]) # Rmed: StepLR\n",
    "    \n",
    "    optimizer_params = {\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [1e-4, 5e-4])\n",
    "    }\n",
    "    \n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.01, 0.05, log=True)\n",
    "        optimizer_params[\"momentum\"] = 0.9 # trial.suggest_float(\"momentum\", 0.8, 0.9)\n",
    "        optimizer_params[\"nesterov\"] = True #bool(trial.suggest_categorical(\"nesterov\", [0, 1]))\n",
    "        optimizer_params[\"weight_decay\"] = 5e-4\n",
    "    else:\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.0001, 0.001, log=True)\n",
    "        optimizer_params[\"weight_decay\"] = 1e-4\n",
    "        \n",
    "    train_transform = transforms.Compose([\n",
    "        # Random Cropping with Padding (Mimics Albumentations `PadIfNeeded`)\n",
    "        transforms.RandomCrop(32, padding=4, fill=0),  \n",
    "        \n",
    "        # Random Horizontal Flip\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  \n",
    "        \n",
    "        # Random Rotation (Limited to Â±15 degrees)\n",
    "        transforms.RandomRotation(degrees=15),  \n",
    "        \n",
    "        # Color Jitter (Brightness, Contrast, Saturation, Hue)\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  \n",
    "        \n",
    "        # Random Affine (Mimics `ShiftScaleRotate`)\n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  \n",
    "        \n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)), # Random Erasing (Mimics `Cutout`)\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_loader, valid_loader = get_cifar10_dataloaders(\n",
    "        train_transform,\n",
    "        subset_percent=1, \n",
    "        valid_size=0.1,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=4,\n",
    "        use_kaggle=False\n",
    "    )\n",
    "\n",
    "    scheduler_params = {}\n",
    "    if scheduler_type == \"StepLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 150, 200)\n",
    "        scheduler_params[\"step_size\"] = trial.suggest_int(\"step_size\", 5, 20)\n",
    "        scheduler_params[\"gamma\"] = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "        \n",
    "    elif scheduler_type == \"CosineAnnealingLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 100, 150)\n",
    "        scheduler_params[\"T_max\"] = num_epochs #trial.suggest_int(\"T_max\", 10, 50)\n",
    "        scheduler_params[\"eta_min\"] = trial.suggest_float(\"eta_min\", 0.0, 1e-6)\n",
    "        \n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 75, 125)\n",
    "        scheduler_params[\"factor\"] = trial.suggest_float(\"factor\", 0.1, 0.9)\n",
    "        scheduler_params[\"patience\"] = trial.suggest_int(\"patience\", 2, 10)\n",
    "        scheduler_params[\"mode\"] = \"min\"\n",
    "        \n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 50, 75)\n",
    "        scheduler_params[\"max_lr\"] = 0.1\n",
    "        scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n",
    "        scheduler_params[\"epochs\"] = num_epochs\n",
    "\n",
    "    # Define model\n",
    "    model = ResNet18()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    trial_details = trial.params.copy()\n",
    "    trial_details[\"model_name\"] =  \"resnet18\"\n",
    "    trial_details[\"trainable_parameters\"] = num_params(model)\n",
    "    \n",
    "    # Print trial details\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{trial.number=}\")\n",
    "    for param, val in trial_details.items():\n",
    "        print(f\"{param}: {val}\")\n",
    "    print(\"- \" * 25)\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "\n",
    "    optimizer = optimizer_map[optimizer_type](model.parameters(), **optimizer_params)\n",
    "    scheduler = scheduler_map[scheduler_type](optimizer, **scheduler_params)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Checkpoint the model with the best validation accuracy\n",
    "    chkpt_filename = f\"trial_{trial.number}_val_acc_{best_val_accuracy:.4f}.pth\"\n",
    "    chkpt_fp = os.path.join(study_dir, model_filename)\n",
    "\n",
    "    # Training\n",
    "    best_val_accuracy = train_model(\n",
    "        model, train_loader, criterion, optimizer, \n",
    "        valid_loader=valid_loader, num_epochs=num_epochs, \n",
    "        device=device, scheduler=scheduler, trial=trial, chkpt_fp=chkpt_fp\n",
    "    )\n",
    "    \n",
    "    trial_details[\"best_val_accuracy\"] = best_val_accuracy\n",
    "    trial_details[\"checkpoint_path\"] = model_path\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "    \n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "study_name = f\"study_{timestamp}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume study\n",
    "Helps run more studies since we only have 4 hour time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and continue running trials\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5)  # Run another batch\n",
    "print(\"Continued Study:\")\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize with mean and std of CIFAR-10\n",
    "# ])\n",
    "\n",
    "# Aggressive Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4, fill=0),  \n",
    "    transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(degrees=15),  \n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  \n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)), # Random Erasing (Mimics `Cutout`)\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch [1/30], Batch [350/352], Train Acc: 0.2524 Loss: 1.8914\n",
      "  Validation Accuracy after Epoch 1: 0.3226\n",
      "  Epoch [2/30], Batch [350/352], Train Acc: 0.3479 Loss: 1.5866\n",
      "  Validation Accuracy after Epoch 2: 0.3732\n",
      "  Epoch [3/30], Batch [350/352], Train Acc: 0.4121 Loss: 1.5375\n",
      "  Validation Accuracy after Epoch 3: 0.4736\n",
      "  Epoch [4/30], Batch [350/352], Train Acc: 0.4751 Loss: 1.7242\n",
      "  Validation Accuracy after Epoch 4: 0.4838\n",
      "  Epoch [5/30], Batch [350/352], Train Acc: 0.5137 Loss: 1.3794\n",
      "  Validation Accuracy after Epoch 5: 0.5136\n",
      "  Epoch [6/30], Batch [350/352], Train Acc: 0.5491 Loss: 1.2662\n",
      "  Validation Accuracy after Epoch 6: 0.5108\n",
      "  Epoch [7/30], Batch [350/352], Train Acc: 0.5721 Loss: 1.1457\n",
      "  Validation Accuracy after Epoch 7: 0.5318\n",
      "  Epoch [8/30], Batch [350/352], Train Acc: 0.5942 Loss: 1.1658\n",
      "  Validation Accuracy after Epoch 8: 0.4618\n",
      "  Epoch [9/30], Batch [350/352], Train Acc: 0.6137 Loss: 1.0630\n",
      "  Validation Accuracy after Epoch 9: 0.6036\n",
      "  Epoch [10/30], Batch [350/352], Train Acc: 0.6299 Loss: 1.0323\n",
      "  Validation Accuracy after Epoch 10: 0.6590\n",
      "  Epoch [11/30], Batch [350/352], Train Acc: 0.6475 Loss: 1.0017\n",
      "  Validation Accuracy after Epoch 11: 0.6558\n",
      "  Epoch [12/30], Batch [350/352], Train Acc: 0.6565 Loss: 0.9356\n",
      "  Validation Accuracy after Epoch 12: 0.6402\n",
      "  Epoch [13/30], Batch [350/352], Train Acc: 0.6696 Loss: 0.9092\n",
      "  Validation Accuracy after Epoch 13: 0.6560\n",
      "  Epoch [14/30], Batch [160/352], Train Acc: 0.6766 Loss: 0.9744"
     ]
    }
   ],
   "source": [
    "model = SmallResNet0()\n",
    "num_epochs = 30\n",
    "\n",
    "single_run(\n",
    "    model,\n",
    "    train_transform,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=128,\n",
    "    optimizer_type=\"SGD\",\n",
    "    optimizer_params={\"lr\": 0.01, \"weight_decay\": 5e-4, \"momentum\": 0.9},\n",
    "    scheduler_type=\"CosineAnnealingLR\",\n",
    "    scheduler_params={\"T_max\": num_epochs}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallResNet0()\n",
    "model.to(device)\n",
    "\n",
    "best_checkpoint_fp = \"checkpoints_study_2025-03-10_19-00-59/model_trial_0_val_acc_0.8604.pth\"\n",
    "\n",
    "if not best_checkpoint_fp:\n",
    "    checkpoint_dir = f\"checkpoints_{study_name}\"\n",
    "    with open(os.path.join(checkpoint_dir, \"study_details.json\"), \"r\") as f:\n",
    "        study_details = json.load(f)\n",
    "    best_checkpoint_fp = study_details[str(study.best_trial.number)][\"checkpoint_path\"]\n",
    "\n",
    "# Load the latest checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_fp)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.9128\n"
     ]
    }
   ],
   "source": [
    "from trainer import evaluate_model\n",
    "from data_loader import get_test_dataloader\n",
    "\n",
    "test_loader = get_test_dataloader(use_kaggle=False)\n",
    "acc = evaluate_model(model, test_loader, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on cifar10.1 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.835\n"
     ]
    }
   ],
   "source": [
    "from cifar10_1_dataloader import get_dataloader_10_1\n",
    "dataloader_10_1 = get_dataloader_10_1()\n",
    "\n",
    "acc = evaluate_model(model, dataloader_10_1, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_kaggle_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission file saved.\n"
     ]
    }
   ],
   "source": [
    "# Generate submission file with test data\n",
    "kaggle_test_loader = get_kaggle_test_dataloader()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, in kaggle_test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 67.3k/67.3k [00:00<00:00, 330kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to Deep Learning Spring 2025: CIFAR 10 classification"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import kaggle\n",
    "# kaggle.api.competition_submit(\n",
    "#     file_name=\"submission.csv\",\n",
    "#     message=\"0.9365\",\n",
    "#     competition=\"deep-learning-spring-2025-project-1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPk8i7jiGjSg0feqDTW0l2u",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
