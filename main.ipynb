{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIugLjz-A2Qd"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gk2657/DLSP25-Project1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from data_loader import get_cifar10_dataloaders, get_test_dataloader, get_kaggle_test_dataloader\n",
    "from helper import optimizer_map, scheduler_map, num_params, update_study_details\n",
    "from models import BaseResNet, EfficientNetB0, SmallResNet0, LargeResNet0, SmallResNet1\n",
    "from trainer import train_model\n",
    "from run import single_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4z7iY1pkk2C"
   },
   "source": [
    "Configure the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "e3wMn_41kd5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    study_dir = f\"studies/{trial.study.study_name}\"\n",
    "    os.makedirs(study_dir, exist_ok=True) # Create a directory for checkpoints if it doesn't exist\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_epochs = trial.suggest_int(\"num_epochs\", 50, 200)\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"smallresnet\", \"efficientnet\", \"largeresnet\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256, 512])\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer_type\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"OneCycleLR\", \"ReduceLROnPlateau\"])\n",
    "    \n",
    "    optimizer_params = {}\n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.001, 0.1, log=True)\n",
    "        optimizer_params[\"momentum\"] = trial.suggest_float(\"momentum\", 0.8, 0.99)\n",
    "        optimizer_params[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-5, 5e-4, log=True)\n",
    "        scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"OneCycleLR\", \"ReduceLROnPlateau\"])\n",
    "        optimizer_params[\"nesterov\"] = scheduler_type != \"ReduceLROnPlateau\"\n",
    "    \n",
    "    elif optimizer_type == \"Adam\":\n",
    "        optimizer_params[\"betas\"] = (\n",
    "            trial.suggest_float(\"beta1\", 0.85, 0.95), \n",
    "            trial.suggest_float(\"beta2\", 0.99, 0.999)\n",
    "        )\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "        optimizer_params[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "        if scheduler_type == \"OneCycleLR\":\n",
    "            scheduler_type = None\n",
    "\n",
    "    elif optimizer_type == \"AdamW\":\n",
    "        optimizer_params[\"betas\"] = (\n",
    "            trial.suggest_float(\"beta1\", 0.85, 0.95), \n",
    "            trial.suggest_float(\"beta2\", 0.99, 0.999)\n",
    "        )\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 1e-5, 5e-3, log=True)\n",
    "        optimizer_params[\"weight_decay\"] = trial.suggest_float(\"weight_decay\", 1e-3, 1e-1, log=True)\n",
    "        scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"OneCycleLR\", \"ReduceLROnPlateau\"])\n",
    "\n",
    "    \n",
    "    # train_transform = transforms.Compose([\n",
    "    #     transforms.RandomCrop(32, padding=4),\n",
    "    #     transforms.RandomHorizontalFlip(0.5),\n",
    "    #     transforms.RandomRotation(15),\n",
    "    #     transforms.ToTensor(),\n",
    "    #     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    # ])\n",
    "\n",
    "    # Realistic tranformation for better generalization\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),  # Mild color variations\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "        \n",
    "    train_loader, valid_loader = get_cifar10_dataloaders(\n",
    "        train_transform,\n",
    "        subset_percent=1, \n",
    "        valid_size=0.1,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "        use_kaggle=True\n",
    "    )\n",
    "\n",
    "    scheduler_params = {}\n",
    "    if scheduler_type == \"CosineAnnealingLR\":\n",
    "        # num_epochs = trial.suggest_int(\"num_epochs\", 100, 150)\n",
    "        scheduler_params[\"T_max\"] = num_epochs\n",
    "        scheduler_params[\"eta_min\"] = trial.suggest_float(\"eta_min\", 1e-6, 1e-3, log=True)\n",
    "        \n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        # num_epochs = trial.suggest_int(\"num_epochs\", 75, 125)\n",
    "        scheduler_params[\"factor\"] = trial.suggest_float(\"factor\", 0.1, 0.5)\n",
    "        scheduler_params[\"patience\"] = trial.suggest_int(\"patience\", 5, 20)\n",
    "        scheduler_params[\"threshold\"] = trial.suggest_float(\"factor\", 0.01, 0.1)\n",
    "        scheduler_params[\"mode\"] = \"min\"\n",
    "        \n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        # num_epochs = trial.suggest_int(\"num_epochs\", 50, 75)\n",
    "        if optimizer_type == \"SGD\":\n",
    "            scheduler_params[\"max_lr\"] = trial.suggest_float(\"factor\", 0.01, 0.3)\n",
    "        else: # AdamW\n",
    "            scheduler_params[\"max_lr\"] = trial.suggest_float(\"factor\", 0.001, 0.01)\n",
    "        scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n",
    "        scheduler_params[\"epochs\"] = num_epochs\n",
    "        scheduler_params[\"anneal_strategy\"] = \"cos\"\n",
    "\n",
    "                \n",
    "    # Select Model\n",
    "    if model_type == \"smallresnet\":\n",
    "        model = SmallResNet0()\n",
    "    elif model_type == \"efficientnet\":\n",
    "        model = EfficientNetB0()\n",
    "    elif model_type == \"largeresnet\":\n",
    "        model = LargeResNet0()\n",
    "    else:\n",
    "        model = BaseResNet()\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    trial_details = trial.params.copy()\n",
    "    trial_details[\"trainable_parameters\"] = num_params(model)\n",
    "    \n",
    "    # Print trial details\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{trial.number=}\")\n",
    "    for param, val in trial_details.items():\n",
    "        print(f\"{param}: {val}\")\n",
    "    print(\"- \" * 25)\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "\n",
    "    optimizer = optimizer_map[optimizer_type](model.parameters(), **optimizer_params)\n",
    "    scheduler = scheduler_map[scheduler_type](optimizer, **scheduler_params) if scheduler_type else None\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Checkpoint the model with the best validation accuracy\n",
    "    chkpt_dir = os.path.join(study_dir, \"checkpoint\")\n",
    "    plot_dir = os.path.join(study_dir, \"plots\")\n",
    "    os.makedirs(chkpt_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # Training\n",
    "    best_val_accuracy = train_model(\n",
    "        model, train_loader, criterion, optimizer, valid_loader=valid_loader, num_epochs=num_epochs, \n",
    "        device=device, scheduler=scheduler, trial=trial, chkpt_dir=chkpt_dir, plot_dir=plot_dir\n",
    "    )\n",
    "    \n",
    "    trial_details[\"best_val_accuracy\"] = best_val_accuracy\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "study_name = f\"study_{timestamp}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=25)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume study\n",
    "Helps run more studies since we only have 4 hour time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 19:25:36,469] Using an existing study with name 'study_2025-03-11_16-30-52' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "trial.number=39\n",
      "num_epochs: 107\n",
      "model_type: largeresnet\n",
      "batch_size: 128\n",
      "optimizer_type: Adam\n",
      "scheduler_type: OneCycleLR\n",
      "beta1: 0.9351259225328429\n",
      "beta2: 0.9977488472985936\n",
      "learning_rate: 0.0005225672559860701\n",
      "weight_decay: 7.013916683605919e-06\n",
      "trainable_parameters: 4903242\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "  Epoch [1/107], Batch [350/352], Train Acc: 45.6004 Loss: 1.4767\n",
      "  Validation Accuracy after Epoch 1: 56.3200\n",
      "  Cidar10.1 Accuracy: 46.25\n",
      "  Epoch [2/107], Batch [350/352], Train Acc: 64.2545 Loss: 1.3031\n",
      "  Validation Accuracy after Epoch 2: 64.8800\n",
      "  Cidar10.1 Accuracy: 53.4\n",
      "  Epoch [3/107], Batch [350/352], Train Acc: 72.0357 Loss: 1.2326\n",
      "  Validation Accuracy after Epoch 3: 73.7800\n",
      "  Cidar10.1 Accuracy: 61.75\n",
      "  Epoch [4/107], Batch [350/352], Train Acc: 76.4955 Loss: 1.0721\n",
      "  Validation Accuracy after Epoch 4: 78.2200\n",
      "  Cidar10.1 Accuracy: 64.15\n",
      "  Epoch [5/107], Batch [350/352], Train Acc: 79.2054 Loss: 0.9299\n",
      "  Validation Accuracy after Epoch 5: 78.9200\n",
      "  Cidar10.1 Accuracy: 68.25\n",
      "  Epoch [6/107], Batch [350/352], Train Acc: 81.4353 Loss: 0.9162\n",
      "  Validation Accuracy after Epoch 6: 81.0600\n",
      "  Cidar10.1 Accuracy: 69.55\n",
      "  Epoch [7/107], Batch [350/352], Train Acc: 83.2232 Loss: 0.8787\n",
      "  Validation Accuracy after Epoch 7: 79.6400\n",
      "  Cidar10.1 Accuracy: 67.9\n",
      "  Epoch [8/107], Batch [350/352], Train Acc: 84.5045 Loss: 0.9067\n",
      "  Validation Accuracy after Epoch 8: 82.6800\n",
      "  Cidar10.1 Accuracy: 73.55\n",
      "  Epoch [9/107], Batch [350/352], Train Acc: 85.7634 Loss: 0.8920\n",
      "  Validation Accuracy after Epoch 9: 80.8000\n",
      "  Cidar10.1 Accuracy: 71.2\n",
      "  Epoch [10/107], Batch [350/352], Train Acc: 86.7321 Loss: 0.8065\n",
      "  Validation Accuracy after Epoch 10: 84.3600\n",
      "  Cidar10.1 Accuracy: 73.35\n",
      "  Epoch [11/107], Batch [350/352], Train Acc: 87.5491 Loss: 0.8844\n",
      "  Validation Accuracy after Epoch 11: 85.6600\n",
      "  Cidar10.1 Accuracy: 76.75\n",
      "  Epoch [12/107], Batch [350/352], Train Acc: 88.4442 Loss: 0.7951\n",
      "  Validation Accuracy after Epoch 12: 86.0200\n",
      "  Cidar10.1 Accuracy: 77.0\n",
      "  Epoch [13/107], Batch [350/352], Train Acc: 89.2991 Loss: 0.8199\n",
      "  Validation Accuracy after Epoch 13: 86.3400\n",
      "  Cidar10.1 Accuracy: 75.6\n",
      "  Epoch [14/107], Batch [350/352], Train Acc: 89.6116 Loss: 0.8418\n",
      "  Validation Accuracy after Epoch 14: 86.7600\n",
      "  Cidar10.1 Accuracy: 76.85\n",
      "  Epoch [15/107], Batch [350/352], Train Acc: 90.4688 Loss: 0.7649\n",
      "  Validation Accuracy after Epoch 15: 85.9200\n",
      "  Cidar10.1 Accuracy: 76.0\n",
      "  Epoch [16/107], Batch [350/352], Train Acc: 90.9754 Loss: 0.7344\n",
      "  Validation Accuracy after Epoch 16: 85.9400\n",
      "  Cidar10.1 Accuracy: 75.8\n",
      "  Epoch [17/107], Batch [350/352], Train Acc: 91.5737 Loss: 0.7503\n",
      "  Validation Accuracy after Epoch 17: 85.8400\n",
      "  Cidar10.1 Accuracy: 77.55\n",
      "  Epoch [18/107], Batch [350/352], Train Acc: 91.8125 Loss: 0.7038\n",
      "  Validation Accuracy after Epoch 18: 87.9200\n",
      "  Cidar10.1 Accuracy: 78.75\n",
      "  Epoch [19/107], Batch [350/352], Train Acc: 92.4196 Loss: 0.6978\n",
      "  Validation Accuracy after Epoch 19: 87.9400\n",
      "  Cidar10.1 Accuracy: 77.8\n",
      "  Epoch [20/107], Batch [350/352], Train Acc: 92.8504 Loss: 0.6666\n",
      "  Validation Accuracy after Epoch 20: 89.0200\n",
      "  Cidar10.1 Accuracy: 79.05\n",
      "  Epoch [21/107], Batch [350/352], Train Acc: 93.1116 Loss: 0.6763\n",
      "  Validation Accuracy after Epoch 21: 88.6200\n",
      "  Cidar10.1 Accuracy: 79.95\n",
      "  Epoch [22/107], Batch [350/352], Train Acc: 93.5558 Loss: 0.6243\n",
      "  Validation Accuracy after Epoch 22: 87.4600\n",
      "  Cidar10.1 Accuracy: 78.3\n",
      "  Epoch [23/107], Batch [350/352], Train Acc: 93.9821 Loss: 0.6999\n",
      "  Validation Accuracy after Epoch 23: 86.9600\n",
      "  Cidar10.1 Accuracy: 79.15\n",
      "  Epoch [24/107], Batch [350/352], Train Acc: 94.3103 Loss: 0.6509\n",
      "  Validation Accuracy after Epoch 24: 89.2400\n",
      "  Cidar10.1 Accuracy: 82.1\n",
      "  Epoch [25/107], Batch [350/352], Train Acc: 94.4665 Loss: 0.7237\n",
      "  Validation Accuracy after Epoch 25: 88.4400\n",
      "  Cidar10.1 Accuracy: 79.95\n",
      "  Epoch [26/107], Batch [350/352], Train Acc: 94.8996 Loss: 0.6121\n",
      "  Validation Accuracy after Epoch 26: 89.5000\n",
      "  Cidar10.1 Accuracy: 82.1\n",
      "  Epoch [27/107], Batch [350/352], Train Acc: 94.9933 Loss: 0.6322\n",
      "  Validation Accuracy after Epoch 27: 89.8000\n",
      "  Cidar10.1 Accuracy: 83.0\n",
      "  Epoch [28/107], Batch [350/352], Train Acc: 95.3728 Loss: 0.5851\n",
      "  Validation Accuracy after Epoch 28: 88.8800\n",
      "  Cidar10.1 Accuracy: 81.2\n",
      "  Epoch [29/107], Batch [350/352], Train Acc: 95.6094 Loss: 0.6203\n",
      "  Validation Accuracy after Epoch 29: 89.8000\n",
      "  Cidar10.1 Accuracy: 81.8\n",
      "  Epoch [30/107], Batch [350/352], Train Acc: 95.8750 Loss: 0.6266\n",
      "  Validation Accuracy after Epoch 30: 89.8400\n",
      "  Cidar10.1 Accuracy: 81.35\n",
      "  Epoch [31/107], Batch [350/352], Train Acc: 96.0580 Loss: 0.6206\n",
      "  Validation Accuracy after Epoch 31: 89.4200\n",
      "  Cidar10.1 Accuracy: 82.05\n",
      "  Epoch [32/107], Batch [350/352], Train Acc: 96.0670 Loss: 0.5675\n",
      "  Validation Accuracy after Epoch 32: 88.6000\n",
      "  Cidar10.1 Accuracy: 81.55\n",
      "  Epoch [33/107], Batch [350/352], Train Acc: 96.4576 Loss: 0.6088\n",
      "  Validation Accuracy after Epoch 33: 89.6400\n",
      "  Cidar10.1 Accuracy: 82.7\n",
      "  Epoch [34/107], Batch [350/352], Train Acc: 96.6406 Loss: 0.5952\n",
      "  Validation Accuracy after Epoch 34: 90.0800\n",
      "  Cidar10.1 Accuracy: 81.75\n",
      "  Epoch [35/107], Batch [350/352], Train Acc: 96.6272 Loss: 0.6421\n",
      "  Validation Accuracy after Epoch 35: 89.4200\n",
      "  Cidar10.1 Accuracy: 82.5\n",
      "  Epoch [36/107], Batch [350/352], Train Acc: 96.9353 Loss: 0.5701\n",
      "  Validation Accuracy after Epoch 36: 89.8800\n",
      "  Cidar10.1 Accuracy: 82.35\n",
      "  Epoch [37/107], Batch [350/352], Train Acc: 97.0759 Loss: 0.5803\n",
      "  Validation Accuracy after Epoch 37: 90.5000\n",
      "  Cidar10.1 Accuracy: 82.3\n",
      "  Epoch [38/107], Batch [350/352], Train Acc: 97.1607 Loss: 0.6060\n",
      "  Validation Accuracy after Epoch 38: 90.3800\n",
      "  Cidar10.1 Accuracy: 83.6\n",
      "  Epoch [39/107], Batch [140/352], Train Acc: 97.5391 Loss: 0.6214"
     ]
    }
   ],
   "source": [
    "study_name = \"study_2025-03-11_16-30-52\"\n",
    "\n",
    "# Load and continue running trials\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=25)  # Run another batch\n",
    "print(\"Continued Study:\")\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min Augmentation\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "\n",
    "# # Medium Augmentation\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     # transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize with mean and std of CIFAR-10\n",
    "# ])\n",
    "\n",
    "# Realistic tranformation for better generalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),  # Mild color variations\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "\n",
    "# # Aggressive Augmentation\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4, fill=0),  \n",
    "#     transforms.RandomHorizontalFlip(p=0.5),  \n",
    "#     transforms.RandomRotation(degrees=15),  \n",
    "#     transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  \n",
    "#     transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  \n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)), # Random Erasing (Mimics `Cutout`)\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch [1/50], Batch [350/352], Train Acc: 35.8103 Loss: 1.6766\n",
      "  Validation Accuracy after Epoch 1: 48.5000\n",
      "  Cidar10.1 Accuracy: 41.25\n",
      "  Epoch [2/50], Batch [350/352], Train Acc: 52.8326 Loss: 1.5933\n",
      "  Validation Accuracy after Epoch 2: 44.2400\n",
      "  Cidar10.1 Accuracy: 36.45\n",
      "  Epoch [3/50], Batch [350/352], Train Acc: 62.1406 Loss: 1.1996\n",
      "  Validation Accuracy after Epoch 3: 68.5400\n",
      "  Cidar10.1 Accuracy: 56.9\n",
      "  Epoch [4/50], Batch [350/352], Train Acc: 68.7746 Loss: 1.2148\n",
      "  Validation Accuracy after Epoch 4: 65.1600\n",
      "  Cidar10.1 Accuracy: 54.65\n",
      "  Epoch [5/50], Batch [350/352], Train Acc: 72.5781 Loss: 1.1803\n",
      "  Validation Accuracy after Epoch 5: 75.4200\n",
      "  Cidar10.1 Accuracy: 64.25\n",
      "  Epoch [6/50], Batch [350/352], Train Acc: 75.6429 Loss: 1.0801\n",
      "  Validation Accuracy after Epoch 6: 71.8400\n",
      "  Cidar10.1 Accuracy: 61.0\n",
      "  Epoch [7/50], Batch [350/352], Train Acc: 77.6473 Loss: 0.9669\n",
      "  Validation Accuracy after Epoch 7: 79.2800\n",
      "  Cidar10.1 Accuracy: 68.5\n",
      "  Epoch [8/50], Batch [350/352], Train Acc: 79.3482 Loss: 0.9810\n",
      "  Validation Accuracy after Epoch 8: 76.9400\n",
      "  Cidar10.1 Accuracy: 64.85\n",
      "  Epoch [9/50], Batch [350/352], Train Acc: 80.8281 Loss: 0.9407\n",
      "  Validation Accuracy after Epoch 9: 81.2400\n",
      "  Cidar10.1 Accuracy: 70.1\n",
      "  Epoch [10/50], Batch [350/352], Train Acc: 81.5045 Loss: 0.9501\n",
      "  Validation Accuracy after Epoch 10: 74.7400\n",
      "  Cidar10.1 Accuracy: 64.0\n",
      "  Epoch [11/50], Batch [350/352], Train Acc: 82.9442 Loss: 0.7173\n",
      "  Validation Accuracy after Epoch 11: 82.2600\n",
      "  Cidar10.1 Accuracy: 72.8\n",
      "  Epoch [12/50], Batch [350/352], Train Acc: 83.5402 Loss: 0.8575\n",
      "  Validation Accuracy after Epoch 12: 80.1000\n",
      "  Cidar10.1 Accuracy: 68.15\n",
      "  Epoch [13/50], Batch [350/352], Train Acc: 84.4353 Loss: 0.9592\n",
      "  Validation Accuracy after Epoch 13: 82.0800\n",
      "  Cidar10.1 Accuracy: 71.45\n",
      "  Epoch [14/50], Batch [350/352], Train Acc: 84.8259 Loss: 0.8031\n",
      "  Validation Accuracy after Epoch 14: 80.7600\n",
      "  Cidar10.1 Accuracy: 72.2\n",
      "  Epoch [15/50], Batch [350/352], Train Acc: 85.6049 Loss: 0.6805\n",
      "  Validation Accuracy after Epoch 15: 83.6800\n",
      "  Cidar10.1 Accuracy: 73.25\n",
      "  Epoch [16/50], Batch [350/352], Train Acc: 85.8750 Loss: 0.8820\n",
      "  Validation Accuracy after Epoch 16: 83.1000\n",
      "  Cidar10.1 Accuracy: 73.5\n",
      "  Epoch [17/50], Batch [350/352], Train Acc: 86.6853 Loss: 0.8068\n",
      "  Validation Accuracy after Epoch 17: 84.7400\n",
      "  Cidar10.1 Accuracy: 74.15\n",
      "  Epoch [18/50], Batch [350/352], Train Acc: 86.9375 Loss: 0.7830\n",
      "  Validation Accuracy after Epoch 18: 84.3600\n",
      "  Cidar10.1 Accuracy: 74.35\n",
      "  Epoch [19/50], Batch [350/352], Train Acc: 87.8304 Loss: 0.7896\n",
      "  Validation Accuracy after Epoch 19: 81.0800\n",
      "  Cidar10.1 Accuracy: 72.75\n",
      "  Epoch [20/50], Batch [350/352], Train Acc: 87.5379 Loss: 0.8013\n",
      "  Validation Accuracy after Epoch 20: 84.7200\n",
      "  Cidar10.1 Accuracy: 76.6\n",
      "  Epoch [21/50], Batch [350/352], Train Acc: 88.4665 Loss: 0.7479\n",
      "  Validation Accuracy after Epoch 21: 80.5400\n",
      "  Cidar10.1 Accuracy: 71.6\n",
      "  Epoch [22/50], Batch [350/352], Train Acc: 88.4420 Loss: 0.6718\n",
      "  Validation Accuracy after Epoch 22: 85.0800\n",
      "  Cidar10.1 Accuracy: 77.6\n",
      "  Epoch [23/50], Batch [350/352], Train Acc: 89.0982 Loss: 0.7933\n",
      "  Validation Accuracy after Epoch 23: 80.8600\n",
      "  Cidar10.1 Accuracy: 71.5\n",
      "  Epoch [24/50], Batch [350/352], Train Acc: 89.1295 Loss: 0.7145\n",
      "  Validation Accuracy after Epoch 24: 86.5200\n",
      "  Cidar10.1 Accuracy: 77.4\n",
      "  Epoch [25/50], Batch [350/352], Train Acc: 89.4799 Loss: 0.7536\n",
      "  Validation Accuracy after Epoch 25: 83.6800\n",
      "  Cidar10.1 Accuracy: 76.15\n",
      "  Epoch [26/50], Batch [350/352], Train Acc: 89.6295 Loss: 0.7437\n",
      "  Validation Accuracy after Epoch 26: 86.7600\n",
      "  Cidar10.1 Accuracy: 80.3\n",
      "  Epoch [27/50], Batch [350/352], Train Acc: 90.1138 Loss: 0.8112\n",
      "  Validation Accuracy after Epoch 27: 80.8000\n",
      "  Cidar10.1 Accuracy: 73.0\n",
      "  Epoch [28/50], Batch [350/352], Train Acc: 90.1540 Loss: 0.7862\n",
      "  Validation Accuracy after Epoch 28: 87.0200\n",
      "  Cidar10.1 Accuracy: 79.65\n",
      "  Epoch [29/50], Batch [350/352], Train Acc: 90.9219 Loss: 0.8225\n",
      "  Validation Accuracy after Epoch 29: 81.8200\n",
      "  Cidar10.1 Accuracy: 72.05\n",
      "  Epoch [30/50], Batch [350/352], Train Acc: 90.6763 Loss: 0.8217\n",
      "  Validation Accuracy after Epoch 30: 88.0200\n",
      "  Cidar10.1 Accuracy: 80.0\n",
      "  Epoch [31/50], Batch [350/352], Train Acc: 91.0513 Loss: 0.7273\n",
      "  Validation Accuracy after Epoch 31: 83.8800\n",
      "  Cidar10.1 Accuracy: 73.4\n",
      "  Epoch [32/50], Batch [350/352], Train Acc: 91.1786 Loss: 0.7384\n",
      "  Validation Accuracy after Epoch 32: 87.4800\n",
      "  Cidar10.1 Accuracy: 79.8\n",
      "  Epoch [33/50], Batch [350/352], Train Acc: 91.4330 Loss: 0.7899\n",
      "  Validation Accuracy after Epoch 33: 85.2800\n",
      "  Cidar10.1 Accuracy: 78.35\n",
      "  Epoch [34/50], Batch [350/352], Train Acc: 91.7612 Loss: 0.7747\n",
      "  Validation Accuracy after Epoch 34: 88.1400\n",
      "  Cidar10.1 Accuracy: 80.85\n",
      "  Epoch [35/50], Batch [350/352], Train Acc: 91.8415 Loss: 0.7525\n",
      "  Validation Accuracy after Epoch 35: 85.7600\n",
      "  Cidar10.1 Accuracy: 76.75\n",
      "  Epoch [36/50], Batch [350/352], Train Acc: 92.1429 Loss: 0.6597\n",
      "  Validation Accuracy after Epoch 36: 87.7600\n",
      "  Cidar10.1 Accuracy: 81.3\n",
      "  Epoch [37/50], Batch [350/352], Train Acc: 92.0536 Loss: 0.8591\n",
      "  Validation Accuracy after Epoch 37: 85.0600\n",
      "  Cidar10.1 Accuracy: 77.3\n",
      "  Epoch [38/50], Batch [350/352], Train Acc: 92.5603 Loss: 0.6753\n",
      "  Validation Accuracy after Epoch 38: 87.1000\n",
      "  Cidar10.1 Accuracy: 80.25\n",
      "  Epoch [39/50], Batch [350/352], Train Acc: 92.5781 Loss: 0.7317\n",
      "  Validation Accuracy after Epoch 39: 86.7000\n",
      "  Cidar10.1 Accuracy: 79.5\n",
      "  Epoch [40/50], Batch [350/352], Train Acc: 92.7478 Loss: 0.6276\n",
      "  Validation Accuracy after Epoch 40: 87.6600\n",
      "  Cidar10.1 Accuracy: 81.1\n",
      "  Epoch [41/50], Batch [350/352], Train Acc: 92.9107 Loss: 0.7167\n",
      "  Validation Accuracy after Epoch 41: 87.2000\n",
      "  Cidar10.1 Accuracy: 79.75\n",
      "  Epoch [42/50], Batch [350/352], Train Acc: 92.7946 Loss: 0.6507\n",
      "  Validation Accuracy after Epoch 42: 87.1400\n",
      "  Cidar10.1 Accuracy: 79.75\n",
      "  Epoch [43/50], Batch [350/352], Train Acc: 93.0759 Loss: 0.6637\n",
      "  Validation Accuracy after Epoch 43: 87.5800\n",
      "  Cidar10.1 Accuracy: 80.5\n",
      "  Epoch [44/50], Batch [350/352], Train Acc: 93.0737 Loss: 0.6638\n",
      "  Validation Accuracy after Epoch 44: 87.1400\n",
      "  Cidar10.1 Accuracy: 77.95\n",
      "  Epoch [45/50], Batch [350/352], Train Acc: 93.0714 Loss: 0.6077\n",
      "  Validation Accuracy after Epoch 45: 88.0400\n",
      "  Cidar10.1 Accuracy: 80.65\n",
      "  Epoch [46/50], Batch [350/352], Train Acc: 93.5580 Loss: 0.6034\n",
      "  Validation Accuracy after Epoch 46: 87.6200\n",
      "  Cidar10.1 Accuracy: 79.05\n",
      "  Epoch [47/50], Batch [350/352], Train Acc: 93.3705 Loss: 0.6681\n",
      "  Validation Accuracy after Epoch 47: 88.1200\n",
      "  Cidar10.1 Accuracy: 80.65\n",
      "  Epoch [48/50], Batch [350/352], Train Acc: 93.7054 Loss: 0.5621\n",
      "  Validation Accuracy after Epoch 48: 84.6800\n",
      "  Cidar10.1 Accuracy: 76.4\n",
      "  Epoch [49/50], Batch [350/352], Train Acc: 93.8705 Loss: 0.6667\n",
      "  Validation Accuracy after Epoch 49: 88.7600\n",
      "  Cidar10.1 Accuracy: 82.5\n",
      "  Epoch [50/50], Batch [350/352], Train Acc: 93.9799 Loss: 0.6415\n",
      "  Validation Accuracy after Epoch 50: 85.7200\n",
      "  Cidar10.1 Accuracy: 77.45\n",
      "Best Validation Accuracy: 88.7600\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88.76"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallResNet1()\n",
    "num_epochs = 50\n",
    "\n",
    "single_run(\n",
    "    model,\n",
    "    train_transform,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=128,\n",
    "    optimizer_type=\"SGD\",\n",
    "    optimizer_params={\"lr\": 0.01, \"weight_decay\": 5e-4, \"momentum\": 0.9, \"nesterov\": True},\n",
    "    scheduler_type=\"CosineAnnealingLR\",\n",
    "    scheduler_params={\"T_max\": num_epochs},\n",
    "    criterion_params={\"label_smoothing\": 0.1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LargeResNet0()\n",
    "num_epochs = 100\n",
    "lr_min = 1e-6\n",
    "lr_max = 1e-2\n",
    "len_train_dataset = 50_000 * 0.9\n",
    "step_size = (len_train_dataset/64) // 2\n",
    "\n",
    "single_run(\n",
    "    model,\n",
    "    train_transform,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=128,\n",
    "    optimizer_type=\"SGD\",\n",
    "    optimizer_params={\"lr\": lr_min, \"momentum\": 0.9, \"nesterov\": True},\n",
    "    scheduler_type=\"CyclicLR\",\n",
    "    scheduler_params={\n",
    "        \"base_lr\": lr_min, \"max_lr\": lr_max, \"step_size_up\": step_size, \n",
    "        \"step_size_down\": step_size, \"gamma\": 0.9999, \"mode\": \"exp_range\", \"cycle_momentum\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallResNet0()\n",
    "model.to(device)\n",
    "\n",
    "# best_checkpoint_fp = \"checkpoints_study_2025-03-10_19-00-59/model_trial_0_val_acc_0.8604.pth\"\n",
    "best_checkpoint_fp = \"studies/study_2025-03-11_01-51-41/checkpoint/trial_0_val_acc_SmallResNet_86.7000_2025-03-11_02-15-35.pth\"\n",
    "\n",
    "if not best_checkpoint_fp:\n",
    "    checkpoint_dir = f\"checkpoints_{study_name}\"\n",
    "    with open(os.path.join(checkpoint_dir, \"study_details.json\"), \"r\") as f:\n",
    "        study_details = json.load(f)\n",
    "    best_checkpoint_fp = study_details[str(study.best_trial.number)][\"checkpoint_path\"]\n",
    "\n",
    "# Load the latest checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_fp)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 83.23\n"
     ]
    }
   ],
   "source": [
    "from trainer import evaluate_model\n",
    "from data_loader import get_test_dataloader\n",
    "\n",
    "test_loader = get_test_dataloader(use_kaggle=True)\n",
    "acc, _ = evaluate_model(model, test_loader, device=device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on cifar10.1 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 72.5\n"
     ]
    }
   ],
   "source": [
    "from cifar10_1_dataloader import get_dataloader_10_1\n",
    "dataloader_10_1 = get_dataloader_10_1()\n",
    "\n",
    "acc, _ = evaluate_model(model, dataloader_10_1, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_kaggle_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission file saved.\n"
     ]
    }
   ],
   "source": [
    "# Generate submission file with test data\n",
    "kaggle_test_loader = get_kaggle_test_dataloader()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, in kaggle_test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67.3k/67.3k [00:00<00:00, 330kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to Deep Learning Spring 2025: CIFAR 10 classification"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import kaggle\n",
    "# kaggle.api.competition_submit(\n",
    "#     file_name=\"submission.csv\",\n",
    "#     message=\"0.9365\",\n",
    "#     competition=\"deep-learning-spring-2025-project-1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPk8i7jiGjSg0feqDTW0l2u",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
