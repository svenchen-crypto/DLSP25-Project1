{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# time.sleep(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIugLjz-A2Qd"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gk2657/DLSP25-Project1/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import optuna\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "\n",
    "from data_loader import get_cifar10_dataloaders, get_test_dataloader, get_kaggle_test_dataloader\n",
    "from helper import optimizer_map, scheduler_map, num_params, update_study_details\n",
    "from models import BaseResNet, EfficientNetB0, SmallResNet0\n",
    "from trainer import train_model\n",
    "from run import single_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4z7iY1pkk2C"
   },
   "source": [
    "Configure the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "e3wMn_41kd5K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    study_dir = f\"studies/{trial.study.study_name}\"\n",
    "    os.makedirs(study_dir, exist_ok=True) # Create a directory for checkpoints if it doesn't exist\n",
    "\n",
    "    # Suggest hyperparameters\n",
    "    num_epochs = 200 # trial.suggest_int(\"num_epochs\", 20, 35)\n",
    "    model_type = trial.suggest_categorical(\"model_type\", [\"smallresnet\", \"efficientnet\"])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "    optimizer_type = trial.suggest_categorical(\"optimizer_type\", [\"Adam\", \"SGD\"]) # Rmed: RMSprop\n",
    "    scheduler_type = trial.suggest_categorical(\"scheduler_type\", [\"CosineAnnealingLR\", \"ReduceLROnPlateau\", \"OneCycleLR\"]) # Rmed: StepLR\n",
    "    \n",
    "    optimizer_params = {\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [1e-4, 5e-4])\n",
    "    }\n",
    "    \n",
    "    if optimizer_type == \"SGD\":\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.01, 0.1, log=True)\n",
    "        optimizer_params[\"momentum\"] = 0.9 # trial.suggest_float(\"momentum\", 0.8, 0.9)\n",
    "        optimizer_params[\"nesterov\"] = True #bool(trial.suggest_categorical(\"nesterov\", [0, 1]))\n",
    "        optimizer_params[\"weight_decay\"] = 5e-4\n",
    "    else:\n",
    "        optimizer_params[\"lr\"] = trial.suggest_float(\"learning_rate\", 0.0001, 0.001, log=True)\n",
    "        optimizer_params[\"weight_decay\"] = 1e-4\n",
    "        \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4, fill=0),  \n",
    "        transforms.RandomHorizontalFlip(p=0.5),  \n",
    "        transforms.RandomRotation(degrees=15),  \n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  \n",
    "        transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_loader, valid_loader = get_cifar10_dataloaders(\n",
    "        train_transform,\n",
    "        subset_percent=1, \n",
    "        valid_size=0.1,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "        use_kaggle=True\n",
    "    )\n",
    "\n",
    "    scheduler_params = {}\n",
    "    if scheduler_type == \"StepLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 150, 200)\n",
    "        scheduler_params[\"step_size\"] = trial.suggest_int(\"step_size\", 5, 20)\n",
    "        scheduler_params[\"gamma\"] = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
    "        \n",
    "    elif scheduler_type == \"CosineAnnealingLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 100, 150)\n",
    "        scheduler_params[\"T_max\"] = num_epochs\n",
    "        \n",
    "    elif scheduler_type == \"ReduceLROnPlateau\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 75, 125)\n",
    "        scheduler_params[\"factor\"] = trial.suggest_float(\"factor\", 0.1, 0.9)\n",
    "        scheduler_params[\"patience\"] = trial.suggest_int(\"patience\", 2, 10)\n",
    "        scheduler_params[\"mode\"] = \"min\"\n",
    "        \n",
    "    elif scheduler_type == \"OneCycleLR\":\n",
    "        num_epochs = trial.suggest_int(\"num_epochs\", 50, 75)\n",
    "        scheduler_params[\"max_lr\"] = 0.1\n",
    "        scheduler_params[\"steps_per_epoch\"] = len(train_loader)\n",
    "        scheduler_params[\"epochs\"] = num_epochs\n",
    "            \n",
    "    # Select Model\n",
    "    if model_type == \"smallresnet\":\n",
    "        model = SmallResNet0()\n",
    "    elif model_type == \"efficientnet\":\n",
    "        model = EfficientNetB0()\n",
    "    else:\n",
    "        model = BaseResNet()\n",
    "        \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    trial_details = trial.params.copy()\n",
    "    trial_details[\"model_name\"] = model.__class__.__name__\n",
    "    trial_details[\"trainable_parameters\"] = num_params(model)\n",
    "    \n",
    "    # Print trial details\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{trial.number=}\")\n",
    "    for param, val in trial_details.items():\n",
    "        print(f\"{param}: {val}\")\n",
    "    print(\"- \" * 25)\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "\n",
    "    optimizer = optimizer_map[optimizer_type](model.parameters(), **optimizer_params)\n",
    "    scheduler = scheduler_map[scheduler_type](optimizer, **scheduler_params)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Checkpoint the model with the best validation accuracy\n",
    "    chkpt_dir = os.path.join(study_dir, \"checkpoint\")\n",
    "    plot_dir = os.path.join(study_dir, \"plots\")\n",
    "    os.makedirs(chkpt_dir, exist_ok=True)\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "    # Training\n",
    "    best_val_accuracy = train_model(\n",
    "        model, train_loader, criterion, optimizer, valid_loader=valid_loader, num_epochs=num_epochs, \n",
    "        device=device, scheduler=scheduler, trial=trial, chkpt_dir=chkpt_dir, plot_dir=plot_dir\n",
    "    )\n",
    "    \n",
    "    trial_details[\"best_val_accuracy\"] = best_val_accuracy\n",
    "    update_study_details(study_dir, trial.number, trial_details)\n",
    "    return best_val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start new study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-11 01:51:42,621] A new study created in RDB with name: study_2025-03-11_01-51-41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "trial.number=0\n",
      "model_type: smallresnet\n",
      "batch_size: 128\n",
      "optimizer_type: Adam\n",
      "scheduler_type: ReduceLROnPlateau\n",
      "weight_decay: 0.0005\n",
      "learning_rate: 0.00032214137835438034\n",
      "num_epochs: 115\n",
      "factor: 0.3825196441327009\n",
      "patience: 6\n",
      "model_name: SmallResNet\n",
      "trainable_parameters: 2998402\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "  Epoch [1/115], Batch [350/352], Train Acc: 29.2232 Loss: 1.8798\n",
      "  Validation Accuracy after Epoch 1: 37.3200\n",
      "  Epoch [2/115], Batch [350/352], Train Acc: 41.5246 Loss: 1.6083\n",
      "  Validation Accuracy after Epoch 2: 44.4000\n",
      "  Epoch [3/115], Batch [350/352], Train Acc: 48.6205 Loss: 1.5436\n",
      "  Validation Accuracy after Epoch 3: 47.8200\n",
      "  Epoch [4/115], Batch [350/352], Train Acc: 53.0580 Loss: 1.6191\n",
      "  Validation Accuracy after Epoch 4: 51.8200\n",
      "  Epoch [5/115], Batch [350/352], Train Acc: 56.9330 Loss: 1.5187\n",
      "  Validation Accuracy after Epoch 5: 54.1400\n",
      "  Epoch [6/115], Batch [350/352], Train Acc: 60.1250 Loss: 1.2948\n",
      "  Validation Accuracy after Epoch 6: 57.7000\n",
      "  Epoch [7/115], Batch [350/352], Train Acc: 62.4576 Loss: 1.3258\n",
      "  Validation Accuracy after Epoch 7: 57.3000\n",
      "  Epoch [8/115], Batch [350/352], Train Acc: 64.6741 Loss: 1.2359\n",
      "  Validation Accuracy after Epoch 8: 62.0600\n",
      "  Epoch [9/115], Batch [350/352], Train Acc: 66.1875 Loss: 1.2882\n",
      "  Validation Accuracy after Epoch 9: 63.5600\n",
      "  Epoch [10/115], Batch [350/352], Train Acc: 67.6830 Loss: 1.1963\n",
      "  Validation Accuracy after Epoch 10: 59.9000\n",
      "  Epoch [11/115], Batch [350/352], Train Acc: 68.8460 Loss: 1.1710\n",
      "  Validation Accuracy after Epoch 11: 66.5800\n",
      "  Epoch [12/115], Batch [50/352], Train Acc: 69.7031 Loss: 1.1978"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "study_name = f\"study_{timestamp}\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5)\n",
    "\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume study\n",
    "Helps run more studies since we only have 4 hour time limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load and continue running trials\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=\"sqlite:///study.db\",\n",
    "    direction=\"maximize\",\n",
    "    load_if_exists=True\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=5)  # Run another batch\n",
    "print(\"Continued Study:\")\n",
    "print(\"Best trial:\", study.best_trial.number)\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best validation accuracy:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(0.5),\n",
    "#     transforms.RandomRotation(15),\n",
    "#     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "#     transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Normalize with mean and std of CIFAR-10\n",
    "# ])\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.RandomCrop(32, padding=4),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "# ])\n",
    "\n",
    "# Aggressive Augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4, fill=0),  \n",
    "    transforms.RandomHorizontalFlip(p=0.5),  \n",
    "    transforms.RandomRotation(degrees=15),  \n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),  \n",
    "    transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3)), # Random Erasing (Mimics `Cutout`)\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Epoch [1/2], Batch [170/176], Train Acc: 21.6797 Loss: 1.9680\n",
      "  Validation Accuracy after Epoch 1: 28.0200\n",
      "  Epoch [2/2], Batch [170/176], Train Acc: 33.6006 Loss: 1.7902\n",
      "  Validation Accuracy after Epoch 2: 38.2200\n",
      "Best Validation Accuracy: 38.2200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38.22,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = EfficientNetB0()\n",
    "# num_epochs = 150\n",
    "\n",
    "# single_run(\n",
    "#     model,\n",
    "#     train_transform,\n",
    "#     num_epochs=num_epochs,\n",
    "#     batch_size=128,\n",
    "#     optimizer_type=\"SGD\",\n",
    "#     optimizer_params={\"lr\": 0.1, \"weight_decay\": 0.0005, \"momentum\": 0.9},\n",
    "#     scheduler_type=\"CosineAnnealingLR\",\n",
    "#     scheduler_params={\"T_max\": num_epochs}\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SmallResNet0()\n",
    "model.to(device)\n",
    "\n",
    "best_checkpoint_fp = \"checkpoints_study_2025-03-10_19-00-59/model_trial_0_val_acc_0.8604.pth\"\n",
    "best_checkpoint_fp = \"checkpoints/SmallResNet_0.7724_2025-03-11_00-58-44.pth\"\n",
    "\n",
    "if not best_checkpoint_fp:\n",
    "    checkpoint_dir = f\"checkpoints_{study_name}\"\n",
    "    with open(os.path.join(checkpoint_dir, \"study_details.json\"), \"r\") as f:\n",
    "        study_details = json.load(f)\n",
    "    best_checkpoint_fp = study_details[str(study.best_trial.number)][\"checkpoint_path\"]\n",
    "\n",
    "# Load the latest checkpoint\n",
    "checkpoint = torch.load(best_checkpoint_fp)\n",
    "model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 69.55\n"
     ]
    }
   ],
   "source": [
    "from trainer import evaluate_model\n",
    "from data_loader import get_test_dataloader\n",
    "\n",
    "test_loader = get_test_dataloader(use_kaggle=False)\n",
    "acc, _ = evaluate_model(model, test_loader, device=device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on cifar10.1 subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 57.7\n"
     ]
    }
   ],
   "source": [
    "from cifar10_1_dataloader import get_dataloader_10_1\n",
    "dataloader_10_1 = get_dataloader_10_1()\n",
    "\n",
    "acc, _ = evaluate_model(model, dataloader_10_1, device)\n",
    "print(\"Acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run model on Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import get_kaggle_test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission file saved.\n"
     ]
    }
   ],
   "source": [
    "# Generate submission file with test data\n",
    "kaggle_test_loader = get_kaggle_test_dataloader()\n",
    "\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, in kaggle_test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images) \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'ID': np.arange(len(predictions)), 'Labels': predictions})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"submission file saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67.3k/67.3k [00:00<00:00, 330kB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Successfully submitted to Deep Learning Spring 2025: CIFAR 10 classification"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import kaggle\n",
    "# kaggle.api.competition_submit(\n",
    "#     file_name=\"submission.csv\",\n",
    "#     message=\"0.9365\",\n",
    "#     competition=\"deep-learning-spring-2025-project-1\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPk8i7jiGjSg0feqDTW0l2u",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
